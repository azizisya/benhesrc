\chapter{Crawling Challenges}\label{chp-challenges}
\section{Crawler Traps}
\subsection{What is a Crawler trap?}
A crawler trap is a site which the crawler may enter and be unable to `escape', meaning that the site keeps generating new links for the crawler to explore. These are most often dynamic sites, where pages are generated in response to each request. Below I have described several common examples of crawler traps:
\begin{itemize}
\item{Recursive symbolic links\\}
A simple crawler trap can be generated by symbolically-linking an entry in a Unix web-accessable directory to the directory itself. The crawler will enter the directory and follow all the links in the directory, recursing infintely deep into the directory structure.
\item{Calendar applications\\}
During test crawls of the gla.ac.uk domain, the crawler discovered several calendar controls on diary or blog type websites. These formed crawler traps as each calendar would have a link to the previous and following months. In one case, the calendar was good until the year 21,000! Obviously having to crawl 20,000 extra URLs with negligible difference in content adds to the crawling time.
\item{Large web-accessable databases\\}
Crawlers can also become trapped in the websites of large databases. During crawls of university domains, I manually blacklisted the sites of the universitys' library. This was usually necessary as the libraries' web site provide access to their databases of books. In the case of the Glasgow University Library, this meant a site with a page for each of the 2 million books in its collection. Two million requests to the same host, with 3 seconds between each (Host Delay), meant that the site would have taken over 70 days to crawl. When deciding to blacklist a site, the crawler operator should take into account how much information would be excluded from the index, preventing users from finding content that site.
\end{itemize}

\subsection{Preventing becoming trapped}\label{sect-preventtrapped}
Labrador has a very flexable suite of URLFilter modules, which allow very fine grained control of those URLs which are allowed and those which are not. These include white and black lists; regular expression matching on any part of the URL; URL Depth; and URL Length. The Depth and Length modules are effective at filtering out crawler traps. For instance, a URL longer than 1000 characters, or more than 20 directories deep, are not likely to be important. I filtered out most calendar traps by matching for URLs containing the word `calendar' in the querystring. For URLFtiler use in a production crawl, refer to Appendix \ref{appndx-config}. \\
\ \\
Finally, I have implemented an additional filtering of URLs at the crawler, which can be configured to allow only the fetching of a given number of URLs that have exactly the same filename but differ in their querystring component - for example http://a/b?p=1 and http://a/b?p=2. This has turned out to be an effective method of preventing the crawler becoming too deeply involved in a large web-accessable database as described above. By inspection we can see that Google performs a similar process, as the query `site:eleanor.lib.gla.ac.uk' states `about 19,100' results, but this is not a limit of pages on any host, as they query `site:www.bbc.co.uk' finds over 600,000 results. The first site is a web-accessable database with about 1 million records accessed using querystring based URLs, while the BBC website contains primarily static pages.\\ \
\\
Even so, a crawl should be monitored to see where the majority of requests are going. Traps can often be identified by inspecting the logs in realtime. My first crawls were monitored all the time, to ensure they were not crawling sites where they were not permitted, or becoming involved in traps.

\section{Duplicate Detection}
\subsection{Why should we detect duplicates?}
Many documents on the Internet are available under different URLs, meaning that the crawler can not detect before visiting a URL whether it has seen the content before or not. This may be because a server has been mirrored in a different location for fault tolerance reasons, or the same server is known under a different name, or the content may have been copied to a different URL and the original not removed.\\ \
\\
All of these conditions can cause the crawler to download the same page at a different location, unless the content can be identified as identical to previously downloaded pages. If the crawler downloads more than one copy of the same page, this will probably lead to each of the identical pages being weighted the same in a query, leading to result pages containing consecutive runs of essentially identical pages, which lowers the precision of the search engine, e.g. the TREC search engine competition penalises duplicates in results. If duplicate pages are identified, only one copy should be saved for indexing. Additionally, links should not be extracted from duplicate pages, as they will lead to URLs that have already been seen (by absolute URLs), or to URLs not seen, but content that has been seen (by relative URLs).\\
\ \\
Commonly, message digests (hashes) are used to identify the content of a document, as it would be too expensive to identify duplicates using the actual content of the documents. There are two forms of digests, described below.
\subsection{Exact-duplicate detection}\label{sect-dupdetection}
Labrador uses the MD5 digest algorithm\cite{rfc1321} to detect exact-duplicate documents. For each document downloaded, the digest is calculated and passed to the dispatcher. The dispatcher takes a note of the digest and sees if it already has the digest, using a hashtable lookup. The dispatcher records the digest if it has not been seen and informs the crawler of the status of the digest. If the dispatcher has seen the digest before, then the document is discarded, any links found are not extracted and the content is not saved.\\
\ \\
The dispatcher uses GDBM's persistent hashtable for storing the digests. This means that the entire hashtable does not need to be kept in memory, while still providing O(log n) lookups.
\subsection{Near-duplicate detection}
Mercator\cite{ref2}, suggest using Broder's\cite{ref11,ref12} method (Shingling) of producing fingerprints for web pages. Broder's fingerprints collision rate (the probability of two different strings having the same fingerprint) is provable, unlike digest algorithms such as MD5 or SHA. More importantly, Broder's fingerprints can be used to detect near-duplicate documents, such as two web pages with the same content, but different links which is clearly an advantage.\\
\ \\
Labrador does not have support for near-duplicate document detection, but provides a ContentFilter interface which would allow a near-duplicate detection module to be easily implemented.
