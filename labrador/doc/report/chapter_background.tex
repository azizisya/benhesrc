\chapter{Background}
\section{Search Engine History}
Although the crawler has been the backbone of World Wide Web search engines for some time, crawlers actually predate its coming. The first crawlers were actually written to crawl FTP and Gopher sites to record filenames or content. However, the first crawler for the web, known as World Wide Web Worm\cite{ref18}, was let loose onto the Internet on 1993. Initially designed purely to count web servers, it was soon modified to record URLs it found, to prevent it revisiting the same page hundreds of times. In fact, this actually noticably slowed the Internet of 1993 and there-in was born the politeness de-facto protocols. The second crawler arrived in October 1993, shortly followed by many, many more.\\
\\ \
By 1994 and 1995, some of the bigger names were appearing on the search engine scene: WebCrawler, Excite, Lycos, Infoseek, Yahoo and then Altavista. Not all provided text search services - Yahoo was a particular example, which did not originally crawl Internet websites, but instead searched the descriptions of manually entered URLs.\\
\ \\
It wasn't until 1998 that any major advances in search technology were made, when the Google founders, while studying for their PhD at Stanford, discovered that the democratic link structure of web pages on the Internet could be exploited to determine the importance of a page. This was done by counting the number of in-links to a web page. Their algorithm, known as PageRank\cite{Lawrence981} is the basis of the Google seach engine\cite{ref7}, which they commercialised the following year. Google uses PageRank to rank the results of queries\cite{ref7} and to order the URL queues of its crawlers\cite{ref5}. Five years later, Google is now the most popular search engine in the world, indexing over 4 billion web pages\cite{site6} and fielding more than 200 million queries every day\cite{site7}.

\section{Crawler Technology}
Altavista is of special interest to us, as its original financiers Digital Equipment Corp.(DEC), had the resources to fund the development of Altavista. Because of this, many papers were released by DEC and their successor Compaq on search engine technology. The paper on the Mercator crawler\cite{ref2} has provided the knowledge base to many subsequent crawlers. Mercator, due to the size of the hardware DEC could donate to the project, was built as a crawler on a single machine.\\
\ \\ 
In comparision, Google describes its architecture\cite{ref7} as being of a distributed nature, using 4 machines in 1998. This is now thought to be about 4000 crawlers, connected through multiple ISPs\cite{site8}, crawling sites that are nearest them, based on the number of network hops. This minimises overall bandwidth usage across the world and connection latency during the crawl.\\
\ \\
Academic papers written on crawlers have primarily originated from Stanford University\cite{ref3,ref5,ref7}, although I found the paper from Polytech University, New York\cite{ref1} of particular interest.

\section{Terrier}\label{terrier_desc}
An EPSRC grant has allowed the Information Retrieval Research Group at the Department of Computing Science to develop Terrier - a toolkit for the rapid development of large-scale Information Retrieval (IR) applications. It is based on a framework for deriving non-parametric probabilistic models for IR. The framework deploys more than 50 Divergence From Randomness\cite{ref14} (DFR) models for term weighting. The term weighting models are derived by measuring the divergence of the actual term distribution from that obtained under a random process. Terrier is the underlying search engine of the University of Glasgow's successful participation to the acclaimed TREC forum (NIST, USA). Apart from refined probabilistic weighting schemes, Terrier includes state-of-the-art modules such as, hyperlink structure analysis, combination of evidence approaches, automatic query expansion/re-formulation\cite{xu96query} methodologies and compression techniques.
