#This is the global configuration file for Labrador Web Crawler


#if omitted, defaults to folder above binary
Base /local/terrier_tmp/macdonch/apps/labrador_dcs/

#what port to run the dispatcher on, default 2680
DispatcherPort 2680

#number of subcrawlers to fork on each machine
ForksPerCrawler 9

#minumum number of seconds between each request to a
#given hostname
HostDelay 0

#time until a robots.txt file expires
RobotsTxtExpiry 25

AdminPassword LczRqz4DmQrqdzWYMchcog

#DispatcherCommsLog data/comms.txt

#where to put a robots.txt cache directory
#defaults to data/robots.txt/
#RobotsTxtCache data/robots.txt/

#hmmm required - the method to obtain rootset URLs
ObtainURLs data/rootsets/rootset_dcs.txt

#URL allocation - eg DFS, BFS etc
#implementation detail - BFS adds to end of main queue,
#DFS adds to start
URLAlloc BFS

#takes from the head of the main queue, and allocates
#each URL to a crawler host
#this only encapsulates data manipulation, not
#storage
CrawlerAlloc Partitioned


#URLFilter occurs as URLs are passed to the dispatcher
#from the subcrawlers, ON THE DISPATCHER
#URLFilter ModuleHandler (params)

#filter out most URLs first (shortcutting other filters)
URLFilter Regexp URL =~ ^http(?:s?)://.+\.dcs\.gla\.ac\.uk

URLFilter File Whitelist Hostname data/whitelists/hosts_dcs.txt
URLFilter File Blacklist Extension data/blacklists/extensions_dcs.txt
URLFilter File Blacklist URL data/blacklists/urls_dcs.txt

#next two are fairly self-explanatory - basically crawler trap protection
URLFilter Length 1024
URLFilter URLDepth 15 

#limit request to same filename with different querystring
#TODO currently only enabled with Manager Partitioned
MaxFilenameRequests 0

DataPersistence Hash GDBM_File
DataPersistence Array Tie::File
#PersistenceLevel 2
PersistenceCheckpointEvery 1200

#end the crawl after successfully downloading MaxCrawlURLs urls
#defaults to 0, which means unlimited
MaxCrawlURLs 0
#MaxCrawlURLs 10

#MaxCrawlerRequests 20

#FROM HERE DOWN IS FOR THE SUBCRAWLERS
Manager Partitioned

#Partition SingleHostDirectory
Partition MultipleHostTopDirectory

#SpiderProxy http://wwwcache.dcs.gla.ac.uk:8080
#SpiderProxyUsername
#SpiderProxyPassword

#required - the HTTP Header name field each spider should give
SpiderName Labrador

#Required
SpiderVersion 0.2

#required - the HTTP Header email field each spider should give
SpiderEmail macdonch@dcs.gla.ac.uk

#List of HTML tags which contain links to follow
#other possible options are img:
FollowTagLinks a area meta link

#content filters
#NB: these must contains ContentTypes and MetaRobots (in that
#order) unless you are absolutely positive you know what you
#are doing
ContentFilter ContentTypes
ContentFilter Binary
ContentFilter MetaRobots
ContentFilter Fingerprint


MD5Fingerprints 1

#content types to index
IndexContentTypeWhitelist text/html text/plain application/xhtml+xml application/postscript application/pdf text/xml 
#application/vnd.ms-powerpoint

#content types to process for links
FollowContentTypeWhitelist text/html application/xhtml+xml text/xml

#ContentHandler HandlerModule (params?)
ContentHandler PreTerrier /local/terrier_tmp/macdonch/crawls/crawl_dcs/saved/

#TODO design content filtering for topic driven crawlers


#SpiderSyncStats timeinseconds to tell the dispatcher your stats
#default 3600 (1 hour)
#stats: Number req done, bytes in, bytes out, CPU seconds used etc, mem footprint?
SpiderSyncStats 60

#todo should these be %h %p rather than $HOSTNAME $PID etc?
SpiderAccessLog /local/terrier_tmp/macdonch/crawls/crawl_dcs/logs/access-%H-%P.log


#SpiderMessageLog /tmp/labrador/message$HOSTNAME-$PID.log

# %a - Remote IP address
# %A - Remote hostname
# %d - sprintf standard date format
# %f - Filename
# %p - Remote port
# %T - Time taken to make request
# %t - Current epoch time
# %m - Request method
# %q - Querystring
# %U - entire URL requested
# %u - URI of request
# %s - HTTP status code
# %S - Protocol scheme (eg http, https)
# %r - Referring URL
# %c - Size of content downloaded uncompressed (excluding headers)
# %C - Size of content downloaded compressed (excluding headers)
# %P - PID of requesting crawler
# %H - the hostname of the requesting crawler
#SpiderAccessLogFormat "%t %d %T %s %c %A %u %q %U"
SpiderAccessLogFormat "%t %s %c %U %M %r"

