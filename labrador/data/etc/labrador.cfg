#This is the global configuration file for Labrador Web Crawler


#if omitted, defaults to folder above binary
Base /local/terrier_tmp/macdonch/labrador/

#what port to run the dispatcher on, default 2680
DispatcherPort 2680

#number of subcrawlers to fork on each machine
ForksPerCrawler 12

#minumum number of seconds between each request to a
#given hostname
HostDelay 0

#time until a robots.txt file expires
RobotsTxtExpiry 25

#Logs everthing the dispatcher communicates (I/O) to file
#Omit to disable
#NB grows large
#DispatcherCommsLog data/comms.txt

#where to put a robots.txt cache directory
#defaults to data/robots.txt/
#RobotsTxtCache data/robots.txt/

#hmmm required - the method to allocate URL 
#TODO not implemented
#hmm ObtainURLs File data/rootset.txt

#URL allocation - eg DFS, BFS etc
#implementation detail - BFS adds to end of main queue,
#DFS adds to start
URLAlloc BFS

#takes from the head of the main queue, and allocates
#each URL to a crawler host
#this only encapsulates data manipulation, not
#storage
#CrawlerAlloc PerHost
CrawlerAlloc AnyHost


#URLFilter occurs as URLs are passed to the dispatcher
#from the subcrawlers, ON THE DISPATCHER
#URLFilter ModuleHandler (params)
#we want DNSlookup to be first
URLFilter DNSLookup #passes to another process across a pipe or a socket to perform async lookups, ignores result
URLFilter File Blacklist data/blacklist.txt
#URLFilter File Whitelist data/whitelist.txt

#next two are fairly self-explanatory - basically crawler trap protection
URLFilter URLDepth 15 
URLFilter Length 1024


URLFilter Regexp ^http://www\.dcs\.gla\.ac\.uk/ #return BAD it $url does NOT match this regexp
#TODO figure out how to share robot's data structures with other things

#how far into a crawl to go, should always come last
#not happy - chicken & egg syndrome
#URLFilter LinkDistance 3

DataPersistence Hash GDBM_File
DataPersistence Array Tie::File
#PersistenceLevel 2
PersistenceCheckpointEvery 1200

#end the crawl after successfully downloading MaxCrawlURLs urls
#defaults to 0, which means unlimited
MaxCrawlURLs 0
#MaxCrawlURLs 10


#FROM HERE DOWN IS FOR THE SUBCRAWLERS

#SpiderProxy http://wwwcache.dcs.gla.ac.uk:8080
#SpiderProxyUsername
#SpiderProxyPassword

#required - the HTTP Header name field each spider should give
SpiderName Labrador

#Required
SpiderVersion 0.01

#required - the HTTP Header email field each spider should give
SpiderEmail macdonch@dcs.gla.ac.uk

#Protocol shemes
#AllowedProtocols http(?:s?)
AllowedProtocols http

#List of HTML tags which contain links to follow
#other possible options are img:
FollowTagLinks a area meta link

#extensions of files to remove from crawl
ExtensionsBlacklist jp(?:e?)g gif png zip xls swf dvi avi movie mpg* mp3 bmp tiff tar\.gz tar css wav gz z sit au hqx exe com url class map ram tgz bin rpm mpg mov avi pdf jar dll wmf ppt wmv


#content types to index
ContentTypeWhitelist text/html text/plain application/xhtml+xml application/postscript application/pdf text/xml 
#application/vnd.ms-powerpoint

#content types to process for links
HTMLContentTypeWhitelist text/html application/xhtml+xml text/xml

#ContentHandler HandlerModule (params?)
ContentHandler PreTerrier /local/terrier_tmp/macdonch/crawl_www_2/saved/

#TODO design content filtering for topic driven crawlers


#SpiderSyncStats timeinseconds to tell the dispatcher your stats
#default 3600 (1 hour)
#stats: Number req done, bytes in, bytes out, CPU seconds used etc, mem footprint?
SpiderSyncStats 60

#todo should these be %h %p rather than $HOSTNAME $PID etc?
#SpiderAccessLog /tmp/labrador/access%H-%P.log
#SpiderAccessLog Standard /local/terrier_tmp/macdonch/crawl_www_2/logs/access%H-%P.log
SpiderAccessLog /local/terrier_tmp/macdonch/crawl_www_2/logs/access%H-%P.log


#SpiderMessageLog /tmp/labrador/message$HOSTNAME-$PID.log

# %a - Remote IP address
# %A - Remote hostname
# %d - sprintf standard date format
# %f - Filename
# %p - Remote port
# %T - Time taken to make request
# %t - Current epoch time
# %m - Request method
# %q - Querystring
# %U - entire URL requested
# %u - URI of request
# %s - HTTP status code
# %S - Protocol scheme (eg http, https)
# %r - Referring URL
# %c - Size of content downloaded uncompressed (excluding headers)
# %C - Size of content downloaded compressed (excluding headers)
# %P - PID of requesting crawler
# %H - the hostname of the requesting crawler
SpiderAccessLogFormat "%d %s %U"
#SpiderAccessLogFormat Standard "%d %s %U"


