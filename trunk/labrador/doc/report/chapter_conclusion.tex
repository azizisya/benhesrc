\chapter{Conclusion}

\section{Objectives}
Labrador is an advanced scalable and flexible crawler framework, that can be used and extended within the Information Retrieval Research Group to allow for future research, in for example, topic-driven crawling.\\
\ \\
The layered model allows new components to be developed as additional functionality, or more advanced modules dropped in to replace other modules. However, the crawler can be completely re-tasked without changing any code and it provides all the fundamental requirements necessary to do up to medium sized crawls, say to crawl the ac.uk domain.\\
\ \\
However, it is still flexible enough to configure, such that an adminstrator could fine tune its crawling strategy and its rich network API provides monitoring provision for tools such as MRTG.
\section{Summary}
This was a personally interesting project for me to persue, and one that I would like to continue to develop. Crawlers are a very challenging design problem, requiring a program to scale from a few hundred URLs to millions of URLs. The research that has been performed in crawlers is limited and many problems have still to be solved, or optimal methods found.\\
\ \\
With the massive growth the Internet has seen in recent years, the information `somewhere out there' must be made as accessable to users as possible. To this end, it is necessary that techniques are continually evolved to fetch as much important information as possible, indexed in the most efficient way for retrieval purposes, to satisfy the multitude of needs of the population in this `Information Age'.
